{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aad541b6",
   "metadata": {},
   "source": [
    "# Solving Inventory control with Exact MDP "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34712a8a",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ead251c",
   "metadata": {},
   "source": [
    "The inventory control problem is a fundamental challenge in supply chain management and operations research. It revolves around the optimization of inventory levels to strike a balance between minimizing costs, such as holding excess inventory, and meeting customer demand without incurring excessive shortages. By formulating the problem as a Markov Decision Process (MDP), various reinforcement learning techniques, such as Q-learning and value iteration, can be employed to derive optimal policies that guide decision-making in restocking and order placement, ultimately enhancing the efficiency and profitability of inventory management systems.\n",
    "\n",
    "In the context of inventory control, the state transition probabilities for a Markov Decision Process (MDP) describe the probabilities of moving from one inventory level to another based on the actions taken (order quantities) and the random demand. \n",
    "\n",
    "\n",
    "Let's define the following variables:\n",
    "\n",
    "- $S_t$ :  Current inventory level at time $t$  \n",
    "- $S_{t+1}$  :  Next inventory level at time   $t+1$ \n",
    "- $A_t$  :  Action (order quantity) taken at time   $t$  \n",
    "- $D_t$  :  Random demand at time   $t$  \n",
    "- $C$ :  Capacity of the inventory (maximum inventory level) \n",
    "- $H$: is the holding cost per unit of inventory \n",
    "- $L$: is the shortage cost per unit of demand \n",
    "\n",
    "\n",
    "\n",
    "The state transition probability formula for the inventory control problem can be represented as:\n",
    "\n",
    "$$\n",
    "P(S_{t+1} | S_t, A_t, D_t) = \\begin{cases}\n",
    "                            1, & \\text{if } S_{t+1} = \\max(0, \\min(C, S_t + A_t - D_t)) \\\\\n",
    "                            0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This formula indicates that the transition probability is 1 if the next state $(S_{t+1})$ is the result of applying the action $(A_t)$ and accounting for the random demand (D_t), while ensuring that the inventory level does not exceed the capacity $(C)$. Otherwise, the transition probability is 0.\n",
    "\n",
    "This formula assumes deterministic transitions. However, it can be adjusted to handle problems that involve uncertain transitions or probabilistic demand.\n",
    "\n",
    " \n",
    "\n",
    "The reward $(R)$ for transitioning from state $(S_t)$ to $(S_{t+1})$ based on action $(A_t)$ and demand $(D_t)$ can be formulated as follows:\n",
    "\n",
    "$$ R(S_t, A_t, D_t) =  - \\big[ H \\cdot \\max(0, S_t - D_t)  +   L \\cdot \\max(0, D_t - S_t) \\big] $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efdb9b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7523bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "894d4534-fc4f-4ca0-b60e-16bd85a57827",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89a7950d-0d27-4c5d-95f1-d11c9acf6a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adeb49a5-aa3d-4cc9-8091-aa49429eff9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "970eb167-a063-460e-b3df-464169cba597",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50a2a508-5c74-4d78-b7f0-25a54e994eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f927d9a6-1c3a-48aa-abbf-5da5f2474cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4324c68c-2ee0-40dc-b7f0-ac6d3e985720",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdp.ValueIteration import ValueIteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f288c8ea-0133-4ed2-8fe3-99039dd8e0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdp.utils.env import make_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "38e50b05-7ce1-41a5-9b51-d09e6979c316",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba416a9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e5ee59b",
   "metadata": {},
   "source": [
    "## Inventory control environment setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7060e33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@attr.s(auto_attribs=True)\n",
    "class InventoryControlEnv(gym.Env):\n",
    "    capacity: int = 20\n",
    "    init_inventory: int = 10\n",
    "    holding_cost: int = 1\n",
    "    shortage_cost: int = 10\n",
    "    max_demand: int = 5\n",
    "    episode_length : int = 500\n",
    "    def __attrs_post_init__(self):\n",
    "        self.action_space = gym.spaces.Discrete(self.capacity + 1)  # Actions represent orders\n",
    "        self.observation_space = gym.spaces.Discrete(self.capacity + 1)  # States represent inventory levels\n",
    "        self.nS = self.capacity + 1\n",
    "        self.P = {}\n",
    "        self._compute_transition_probabilities()\n",
    "\n",
    "        self.state = self.init_inventory\n",
    "        self.current_step = 0\n",
    "\n",
    "    def step(self, action: int) -> tuple:\n",
    "        \"\"\"\n",
    "        Perform one step in the environment.\n",
    "\n",
    "        Args:\n",
    "            action (int): The action representing the order quantity.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Tuple containing next state, reward, done flag, and additional info.\n",
    "        \"\"\"\n",
    "        assert self.action_space.contains(action)\n",
    "\n",
    "        demand = np.random.randint(0, self.max_demand + 1)\n",
    "        reward = -self.holding_cost * max(0, self.state - demand) - self.shortage_cost * max(0, demand - self.state)\n",
    "\n",
    "        next_state = max(0, min(self.capacity, self.state + action - demand))\n",
    "        self.state = next_state\n",
    "        self.current_step += 1\n",
    "\n",
    "        done = self.current_step >= self.episode_length\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def reset(self) -> int:\n",
    "        \"\"\"\n",
    "        Reset the environment to the initial state.\n",
    "\n",
    "        Returns:\n",
    "            int: The initial state (inventory level).\n",
    "        \"\"\"\n",
    "        self.state = self.init_inventory\n",
    "        self.current_step = 0\n",
    "        return self.state\n",
    "\n",
    "    def _compute_transition_probabilities(self):\n",
    "        \"\"\"\n",
    "        Compute transition probabilities for the environment.\n",
    "        \"\"\"\n",
    "        for s in range(self.capacity + 1):\n",
    "            self.P[s] = {}\n",
    "            for a in range(self.capacity + 1):\n",
    "                self.P[s][a] = []\n",
    "                for ns in range(self.capacity + 1):\n",
    "                    next_state = max(0, min(self.capacity, s + a - self.max_demand))\n",
    "                    reward = -self.holding_cost * max(0, next_state - self.max_demand) - self.shortage_cost * max(0, self.max_demand - next_state)\n",
    "                    if ns == next_state:\n",
    "                        self.P[s][a].append((1.0, next_state, reward, False))\n",
    "                    else:\n",
    "                        self.P[s][a].append((0.0, next_state, reward, False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "40ff3bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory_env = InventoryControlEnv( capacity=20, \n",
    "                                    init_inventory=10, \n",
    "                                    holding_cost=1, \n",
    "                                    shortage_cost=10, \n",
    "                                    max_demand=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dce69c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of discrete states: 21\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of discrete states:\", inventory_env.observation_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc9959a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4db4340c",
   "metadata": {},
   "source": [
    "## Solving the inventory control problem "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52afdb52",
   "metadata": {},
   "source": [
    "### Value Iteration "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ebbbe6",
   "metadata": {},
   "source": [
    "#### Synchronous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2907c33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value-iteration (state sweep) converged at iteration #3.\n"
     ]
    }
   ],
   "source": [
    "state_sweep_value_iteration = ValueIteration(env=inventory_env, \n",
    "                                 gamma = 0.9,\n",
    "                                 threshold = 1e-6,\n",
    "                                 max_iterations = 10000,\n",
    "                                 inplace=False)\n",
    "state_sweep_value_iteration.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e3e1e988",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_sweep_optimal_state_values = state_sweep_value_iteration.optimal_state_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cd3e10e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0. ,   0. ,   0. ,   0. ,   0. ,   0. ,   0. ,   0. ,   0. ,\n",
       "         0. ,   0. ,  -1. ,  -2. ,  -3. ,  -4. ,  -5. ,  -6.9,  -8.8,\n",
       "       -10.7, -12.6, -14.5])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_sweep_optimal_state_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e0d2cb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_sweep_optimal_policy = state_sweep_value_iteration.optimal_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "876a45f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10,  9,  8,  7,  6,  5,  4,  3,  2,  1,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_sweep_optimal_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "738dad7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10,  9,  8,  7,  6,  5,  4,  3,  2,  1,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_sweep_optimal_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8486fab6",
   "metadata": {},
   "source": [
    "#### Asynchronous "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4fea46ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value-iteration (in-place) converged at iteration #2.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inplace_value_iteration = ValueIteration(env=inventory_env, \n",
    "                                 gamma = 0.9,\n",
    "                                 threshold = 1e-6,\n",
    "                                 max_iterations = 10000,\n",
    "                                 inplace=True)\n",
    "inplace_value_iteration.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "206f7d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10,  9,  8,  7,  6,  5,  4,  3,  2,  1,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inplace_value_iteration.optimal_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc7b3ad",
   "metadata": {},
   "source": [
    "#### Asycnhronous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaa3839",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_iterator = PolicyIterator(env=inventory_env, synchronous_evaluation=True)\n",
    "asynch_final_policy = policy_iterator.policy_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3dab8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "asynch_final_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13023cbf",
   "metadata": {},
   "source": [
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c2e03b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@attr.s(auto_attribs=True)\n",
    "class QLearningAgent:\n",
    "    env: gym.Env\n",
    "    learning_rate: float = 0.01\n",
    "    discount_factor: float = 0.99\n",
    "    exploration_prob: float = 0.5\n",
    "    q_table: np.ndarray = attr.ib(init=False)\n",
    "\n",
    "    def __attrs_post_init__(self):\n",
    "        self.q_table = np.zeros((self.env.observation_space.n, self.env.action_space.n))\n",
    "\n",
    "    def choose_action(self, state: int) -> int:\n",
    "        \"\"\"Choose an action based on exploration or exploitation.\n",
    "\n",
    "        Args:\n",
    "            state (int): Current state.\n",
    "\n",
    "        Returns:\n",
    "            int: Chosen action.\n",
    "        \"\"\"\n",
    "        if np.random.uniform(0, 1) < self.exploration_prob:\n",
    "            return self.env.action_space.sample()  # Explore randomly\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state, :])  # Exploit learned Q-values\n",
    "\n",
    "    def update_q_table(self, state: int, action: int, reward: float, next_state: int) -> None:\n",
    "        \"\"\"Update Q-values based on the Bellman equation.\n",
    "\n",
    "        Args:\n",
    "            state (int): Current state.\n",
    "            action (int): Chosen action.\n",
    "            reward (float): Reward received.\n",
    "            next_state (int): Next state.\n",
    "        \"\"\"\n",
    "        best_next_action = np.argmax(self.q_table[next_state, :])\n",
    "        self.q_table[state, action] += self.learning_rate * (\n",
    "            reward + self.discount_factor * self.q_table[next_state, best_next_action] - self.q_table[state, action]\n",
    "        )\n",
    "\n",
    "    def train(self, num_episodes: int) -> None:\n",
    "        \"\"\"Train the Q-learning agent.\n",
    "\n",
    "        Args:\n",
    "            num_episodes (int): Number of episodes for training.\n",
    "        \"\"\"\n",
    "        for episode in range(num_episodes):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                action = self.choose_action(state)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                self.update_q_table(state, action, reward, next_state)\n",
    "                state = next_state\n",
    "\n",
    "    def get_optimal_policy(self) -> np.ndarray:\n",
    "        \"\"\"Get the optimal policy based on learned Q-values.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Optimal policy.\n",
    "        \"\"\"\n",
    "        return np.argmax(self.q_table, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "18c986d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = QLearningAgent(env=inventory_env,learning_rate  = 0.01,discount_factor  = 0.99,exploration_prob  = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "898d065a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy: [9 6 5 6 5 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "agent.train(num_episodes=100000)\n",
    "optimal_policy = agent.get_optimal_policy()\n",
    "print(\"Optimal Policy:\", optimal_policy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4ff16309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy Descriptions: ['Order 9', 'Order 6', 'Order 5', 'Order 6', 'Order 5', 'Order 3', 'Order 2', 'Order 1', 'Order 1', 'Order 0', 'Order 0', 'Order 0', 'Order 0', 'Order 0', 'Order 0', 'Order 0', 'Order 0', 'Order 0', 'Order 0', 'Order 0', 'Order 0']\n"
     ]
    }
   ],
   "source": [
    "def get_optimal_policy_description(optimal_policy):\n",
    "    action_descriptions = [\"Order {}\".format(i) for i in range(len(optimal_policy))]\n",
    "    return [action_descriptions[action] for action in optimal_policy]\n",
    "optimal_policy_descriptions = get_optimal_policy_description(optimal_policy)\n",
    "print(\"Optimal Policy Descriptions:\", optimal_policy_descriptions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a7c40f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
